---
title: "PRAC2: Limpieza y análisis de datos"
author: Xiaolin Ye, Javier Galan
date: "Mayo 2022"
output:
    html_document:
      number_sections: yes
      toc: true
      toc_depth: 2
      theme: united
      df_print: paged
    pdf_document:
      number_section: yes
      toc: yes
      toc_depth: 2
lang: es
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval=T, echo=T)

library(ggplot2)
library(reshape2)
```

*****
# Descripción del dataset (completado, a revisars)

El dataset elegidos proviene de Kaggle, describe en él la información de los pasajeros de Titanic (género, edad...), las condiciones de embargue (tipo de clase, cabina..) y si sobrevivió el accidente. 

El principal objetivo para analizar estos datos es la predicción de la supervivencia de los pasajeros, para ello nos facilita un juego de datos de entrenamiento "train.csv" como la base para el aprendizaje supervisado y otro juego de "test.csv" que sirve para evaluar la precisión de la predicción.

Esta tarea también forma parte de una competición que celebra Kaggle donde los participantes suben sus trabajos y compiten por ser la mejor implementación que resuelve la pregunta clave:

¿Qué tipo de pasajero tiene mayor probabilidad de sobrevivir?

##Lectura de los datos:

Para crear nuestro modelo, primero leemos los datos de entrenamiento.
```{r message=FALSE, warning=FALSE}
titanic_train <-read.csv("train.csv",header=T,sep=",")
```

```{r message=FALSE, warning=FALSE}
titanic_test <-read.csv("test.csv",header=T,sep=",")
```

Hacemos un breve análisis de los datos ya que nos interesa tener una idea general de los datos que disponemos. 
## Exploración de la base de datos

Primero calcularemos las dimensiones de nuestra base de datos y analizaremos qué tipos de atributos tenemos.

Calculamos las dimensiones de la base de datos mediante la función dim(). Obtenemos que disponemos de 891 registros o pasajeros (filas) y 12 variables (columnas) para el juego de entrenamiento, 418 registros y 11 variables para el juego de validación.

```{r}
dim(titanic_train)
```

```{r}
dim(titanic_test)
```

¿Cuáles son esas variables? 
```{r}
str(titanic_train)
```

Gracias a la función str() sabemos  si las varibales son categoricas o numéricas:

 * *PassengerId*: int. variable cuantitativo.
    
    Numéro entero ordinal que identifica a cada uno de los pasajeros.
 
 * *Survived*   : int. variable cualitativo.
    
    Código numérico que identifica el estado de supervivencia de los pasajeros tras el accidente
    
 * *Pclass*     : int. variable cuantitativo.
 
    Clase del ticket que posee el pasajero, 1 = primera, 2 = segunda y 3 = tercera
    
 * *Name*       : chr. variable cualitativo.
 
    Nombre del pasajero
    
 * *Sex*        : chr. variable cualitativo.
    
    Género del pasajero
    
 * *Age*        : num. variable cuantitativo.
 
    Edad del pasajero
    
 * *SibSp*      : int. variable cuantitativo.
 
    Numéro de hermanos/pareja a bordo
    
 * *Parch*      : int. variable cuantitativo.
 
    Numéro de padres/hijos a bordo
    
 * *Ticket*     : chr. vairable cualitativo
 
    Identificador del ticket
    
 * *Fare*       : num. variable cuantitativo.
 
    Precio del ticket
    
 * *Cabin*      : chr. variable cualitativo.
 
    Identificador de la cabina
    
 * *Embarked*   : chr. variable cualitativo.

    Puerto de embargue. C = Cherbourg, Q = Queenstown, S = Southampton
    
*****

# Limipeza de datos (parcial, a completar)

Para poder generar un modelo preciso y adecuado, es de vital importancia seleccionar, limpiar y transformar los datos en el caso de que sea necesario. 

Empezamos este apartado seleccionando aquellos atributos/registros que nos van a ser útiles a la hora de construir el modelo.

## Selección de datos

### Registros duplicados

Comprobamos que no hay registros duplicados que eliminar
```{r}
sum(duplicated(titanic_train))
```

### correlación entre variables

#### variables cuantitativas
Para poder identificar qué variables cuantitativas podemos descartar, podemos calcular la correlación entre las variables. 

Por un lado, podes identificar y descartar las variables que no contribuyen apenas a la variable objetivo, por otro lado podemos reconocer variables predictivos que están fuertemente relacionados y usar solo una de ellas.

```{r}

numeric_cor <- cor(titanic_train[,c('PassengerId','Survived','Pclass', 'Age', 'SibSp', 'Parch')],
    titanic_train[,c('PassengerId','Survived','Pclass', 'Age', 'SibSp', 'Parch')], method = 'pearson',use = "pairwise.complete.obs")

melted_cormat <- melt(numeric_cor)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2)) + 
  geom_tile(aes(fill = value))+
  geom_text(aes(label = round(value,1 ))) +
  scale_fill_gradient(low = "white", high = "red") 
```

De aquí podemos observar que, a pesar de que PassengerId es un número, es meramente un identificador ordinal que no aporta información particular de los pasajeros y por lo tanto, afecta de forma escasa en todas las demás variables.

Por otro lado, se observa algo de correlación entre las variables SibSp y Parch, que por nuestra intuición, sabemos que una persona con pareja es más probable tener hijos por ejemplo. No obstante, como no se relacionan igual a la variable target, no podemos descartar ninguna de ellas.

#### variables cualitativas

Sabemos que el nombre de una persona viene de fuentes variadas y suele condicionar en los sucesos cotidianos de una forma determinista, por lo que no la incluiremos en nuestro modelo. 

De igual manera a _PassengerId_, _Ticket_, este identificador vienen condicionados por otros factores que no suele relacionarse con los individuos directamente (como por ejemplo la empresa comercializadora, el puerto de embargue, la fecha de compra...), también lo descartaremos.

Puesto que no tenemos más suposiciones lógicas sobre las demás variables, las vamos a mantener para los siguientes pasos de procesado/análisis.

```{r}

titanic_train <- titanic_train[,c(2,3,5,6,7,8,10,11,12)]
```


## Valores nulos

Es de gran interés saber si tenemos muchos valores nulos (campos vacíos) y la distribución de valores por variables. Es por ello recomendable empezar el análisis con una visión general de las variables. 

Mostraremos para cada atributo la cantidad de valores perdidos mediante la función summary.  

```{r}
summary(titanic_train)
```

Por otro lado, también necesitamos encontrar los valores vacíos, podemos usar colSums para saber la suma en cada columna.
```{r}

colSums(titanic_train == "")
```

## Valores extremos

# Análisis de los datos

## Selección de grupos
## Test de normalidad y heteroscedasticidad
## Pruebas estadísticas

# Conclusiones
# Código y exportación de datos
# Recursos básicos

Material didáctico de:  Tipologia y Ciclo de Datos

Complementarios:

* Los descritos para la anterior PEC.
* Fichero :train.csv :Titanic: Machine Learning from Disaster (https://www.kaggle.com/c/titanic)



**Objetivos:**

*1. Descripción del dataset. ¿Por qué es importante y qué pregunta/problema pretende responder?*

*2. Integración y selección de los datos de interés a analizar. Puede ser el resultado de            adicionar    diferentes datasets o una subselección útil de los datos originales, en base al     objetivo que      se quiera conseguir*

*3.Limpieza de los datos.*
3.1.     ¿Los datos contienen ceros o elementos vacíos? Gestiona cada uno de estos casos.
3.2.     Identifica y gestiona los valores extremos.

*4.     Análisis de los datos.*
4.1.     Selección de los grupos de datos que se quieren analizar/comparar (p. e., si se van a        comparar grupos de datos, ¿cuáles son estos grupos y qué tipo de análisis se van a aplicar?)
4.2.     Comprobación de la normalidad y homogeneidad de la varianza.
4.3.     Aplicación de pruebas estadísticas para comparar los grupos de datos. En función de los     datos y el objetivo del estudio, aplicar pruebas de contraste de hipótesis, correlaciones,        regresiones, etc. Aplicar al menos tres métodos de análisis diferentes.

*5.  Representación de los resultados a partir de tablas y gráficas. Este apartado se puede   responder   a   lo   largo   de   la   práctica,   sin   necesidad   de   concentrar   todas   las representaciones en este punto de la práctica.* 

  
*6.     Resolución  del  problema.  A  partir  de  los  resultados  obtenidos,  ¿cuáles  son  las   conclusiones? ¿Los resultados permiten responder al problema?*
  
*7.     Código: Hay que adjuntar el código, preferiblemente en R, con el que se ha realizado la        limpieza, análisis y representación de los datos. Si lo preferís, también podéis trabajar     en Python.*





