---
title: "PRAC2"
output:
  html_document:
    df_print: paged
---

author: Xiaolin Ye, Javier Galan
date: "Mayo 2022"
output:  html_document
     
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval=T, echo=T)
```

******
# Recursos básicos
******

Material didáctico de:  Tipologia y Ciclo de Datosa

Complementarios:

* Los descritos para la anterior PEC.
* Fichero :train.csv :Titanic: Machine Learning from Disaster (https://www.kaggle.com/c/titanic)

******
# Desarrollo práctica
******


**Objetivos:**

*1. Descripción del dataset. ¿Por qué es importante y qué pregunta/problema pretende responder?*

*2. Integración y selección de los datos de interés a analizar. Puede ser el resultado de            adicionar    diferentes datasets o una subselección útil de los datos originales, en base al     objetivo que      se quiera conseguir*

*3.Limpieza de los datos.*
3.1.     ¿Los datos contienen ceros o elementos vacíos? Gestiona cada uno de estos casos.
3.2.     Identifica y gestiona los valores extremos.

*4.     Análisis de los datos.*
4.1.     Selección de los grupos de datos que se quieren analizar/comparar (p. e., si se van a        comparar grupos de datos, ¿cuáles son estos grupos y qué tipo de análisis se van a aplicar?)
4.2.     Comprobación de la normalidad y homogeneidad de la varianza.
4.3.     Aplicación de pruebas estadísticas para comparar los grupos de datos. En función de los     datos y el objetivo del estudio, aplicar pruebas de contraste de hipótesis, correlaciones,        regresiones, etc. Aplicar al menos tres métodos de análisis diferentes.

*5.  Representación de los resultados a partir de tablas y gráficas. Este apartado se puede   responder   a   lo   largo   de   la   práctica,   sin   necesidad   de   concentrar   todas   las representaciones en este punto de la práctica.* 

  
*6.     Resolución  del  problema.  A  partir  de  los  resultados  obtenidos,  ¿cuáles  son  las   conclusiones? ¿Los resultados permiten responder al problema?*
  
*7.     Código: Hay que adjuntar el código, preferiblemente en R, con el que se ha realizado la        limpieza, análisis y representación de los datos. Si lo preferís, también podéis trabajar     en Python.*

## Análisis inicial

#Lectura de los datos:

```{r message=FALSE, warning=FALSE}
datos_titanic <-read.csv("train.csv",header=T,sep=",")
attach(datos_titanic)
```



Empezaremos haciendo un breve análisis de los datos ya que nos interesa tener una idea general de los datos que disponemos. 

### Exploración de la base de datos

Primero calcularemos las dimensiones de nuestra base de datos y analizaremos qué tipos de atributos tenemos.

Para empezar, calculamos las dimensiones de la base de datos mediante la función dim(). Obtenemos que disponemos de 891 registros o pasajeros (filas) y 12 variables (columnas). 

```{r}
dim(datos_titanic)
```

¿Cuáles son esas variables? 
```{r}
str(datos_titanic)
```



Gracias a la función str() sabemos  si las varibales son categoricas o discretas:
_______________


### Descripcion del Daset: (*ampliar*)

 * PassengerId: int  
 * Survived   : int  
 * Pclass     : int   
 * Name       : chr  
 * Sex        : chr  
 * Age        : num  
 * SibSp      : int  
 * Parch      : int  
 * Ticket     : chr  
 * Fare       : num  
 * Cabin      : chr  
 * Embarked   : chr  




Es de gran interés saber si tenemos muchos valores nulos (campos vacíos) y la distribución de valores por variables. Es por ello recomendable empezar el análisis con una visión general de las variables. Mostraremos para cada atributo la cantidad de valores perdidos mediante la función summary.  


```{r}
summary(datos_titanic)
```



